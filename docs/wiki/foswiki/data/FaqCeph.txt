%META:TOPICINFO{author="roman" comment="reprev" date="1627895777" format="1.1" reprev="1" version="1"}%
%META:TOPICPARENT{name="ListOfFaq"}%
---+!! Работа с Ceph

%TOC%

---++ Общие команды Ceph

   * Просмотр информации о соостоянии кластера: ceph health, ceph health detail, ceph -w, я использую: watch -n 5 ceph -s)
   * Свободное место в кластере по пулам: ceph df
   * Список пулов: rados lspools
   * Посмотреть список имэджей со снапшотами: rbd --pool ПУЛ ls -l (работает только с rbd пулами one и hdd-pool)
   * Экспортировать имэдж: rbd export --export-format qcow2 -p ПУЛ ИМЯИМЭДЖА ПУТЬСИМЕНЕМФАЙЛА
   * Прибиндить имэдж к /dev/rbd и работать как с блочным устройством: rbd map ПУЛ/ИМЭДЖ (rbd map one/one-204)
   * Отбиндить:  rbd unmap /dev/rbd0 

%RED%Работать с имэджами только если они не используются или монтировать на RO, иначе ФС на нем побьётся.%ENDCOLOR%

---++ Работа с OSD

---+++ Для просмотра статуса всех osd
ceph osd tree

---+++ Для просмотра свободного места на них
ceph osd df

---+++ Если проводится запланированная замена OSD 
то, чтобы избежать ненужного ребаланса, надо сделать: ceph osd set noout

---+++ Для удаления osd
запускаем скрипт 'rm-osd.sh НОМЕРOSD' на соответствующем сервере. 
Скрипт полуавтоматический, будет спрашивать о каждом действии. 
Перед удалением osd желательно знать имя диска для проверки то ли удаляем.

---+++ Для добавления osd
заходим на fog.

cd /etc/ceph

ceph-deploy --overwrite-conf osd create ИМЯСЕРВЕРА --data ИМЯДИСКА

иногда неправильно определяется типа osd (hdd вместо ssd). Для этого надо сделать

ceph osd crush rm-device-class НОМЕРОСД

ceph osd crush set-device-class ssd НОМЕРОСД

---+++ Вывести OSD из кластера

ceph osd НОМЕР out

Никогда не пытаться выводить несоклько OSD за раз. Будет очень-очень-очень долгий ребилд и если в процессе что-то екнутся, может случится что-то нехорошее. Ну вобщем поступать как с большим рейдом. 

---++ Влить большой имэдж через опеннебулу в ceph

Иногда нужно влить большой имэдж через опеннебулу в ceph.
Но через web это адское занятие, поэтому делается все иначе:

   1 Создаем в opennebula пустой имэдж в нужном пуле (SSD -> one,HDD-> hdd-pool, ($POOL)) и записываем id ($ID).
   2 Идем на любой сервер с ceph (я юзаю fog) 
   3 Скачиваем толстый имэдж ($FILE)
   4 Ищем созданный opennebula пустой имэдж через: rbd ls -l --pool $POOL | grep one-$ID
   5 Если с id и размером все ок, то делаем: rbd rm $POOL/one-$ID
   6 Делаем rbd import $FILE $POOL/one-$ID

---++ Блочные бэкапы Ceph RBD делаются с помощью backy2 на сервере fog (192.168.13.98)

/usr/local/scripts/backy/backup-total.sh
   1 генерация списка rbd устройств через Opennebula API
   2 Создание инкрементальных бэкапов
   3 Очистка старых бэкапов

Там же есть скрипт scrub.sh, который проверяет целостность бэкапов.

Бэкапы делаются каждую ночь.

При добавлении толстого или ненужного для бэкапа имэджа, его id надо указать в /usr/local/scripts/backy/sub/regen_backup.pl - %skipdisk

Если более прямые варианты (фильтрация по меткам, имени имэджа), но это без надобности, так как редко происходит.

---++ Links

   * Заметки о Ceph - https://ceph-docs.readthedocs.io/ru/latest/
   * Ceph Benchmark - https://access.redhat.com/documentation/en-us/red_hat_ceph_storage/1.3/html/administration_guide/benchmarking_performance
   * Mastering Ceph (rus) - https://access.redhat.com/documentation/en-us/red_hat_ceph_storage/1.3/html/administration_guide/benchmarking_performance
   * Ceph performance - https://yourcmc.ru/wiki/index.php?title=Ceph_performance&mobileaction=toggle_view_desktop#General_benchmarking_principles
   * Ceph performance analyze - http://telekomcloud.github.io/ceph/2014/02/26/ceph-performance-analysis_fio_rbd.html
   * Ceph PGs - https://docs.ceph.com/en/latest/rados/operations/placement-groups/?
   * Nagios - https://nagios.sovtest.ru/cgi-bin//status.cgi?hostgroup=Cloud&style=detail&limit=0&limit=0
   * Ceph-deploy (не обновляется) - https://github.com/ceph/ceph-deploy
